
# Semantic Tokenization & Classification of Bank Transactions

This project demonstrates how to improve transaction classification by preprocessing text-based descriptions using rule-based token replacement. Rare or noisy vendor/location tokens are replaced with general class tokens such as `[restaurant]`, `[location]`, and `[date]` to enhance model generalization.

---

## Project Overview

**Objective:**
Transform transaction descriptions into standardized formats using semantic tokens to reduce vocabulary noise and improve classification accuracy.

---

## Project Structure

```
bank-transaction-classifier/
â”œâ”€â”€ data/                         # Data files
â”‚   â”œâ”€â”€ transactions.csv          # Raw dataset
â”‚   â””â”€â”€ transactions_tokenized.csv  # Preprocessed dataset with tokenized descriptions
â”‚
â”œâ”€â”€ notebooks/                    # Development and analysis notebooks
â”‚   â”œâ”€â”€ 01_eda.ipynb              # Initial data exploration
â”‚   â”œâ”€â”€ 02_token_replacement.ipynb # Token development and pattern testing
â”‚   â”œâ”€â”€ 03_apply_tokenizer.ipynb # Applying preprocessing to full dataset
â”‚   â””â”€â”€ 04_model_comparison.ipynb # Model performance: raw vs tokenized
â”‚
â”œâ”€â”€ report/
â”‚   â””â”€â”€ writeup.md                # Findings, assumptions, and detailed explanation
â”‚
â”œâ”€â”€ src/                          # Core preprocessing code
â”‚   â”œâ”€â”€ preprocess.py             # Token replacement logic
â”‚   â””â”€â”€ utils.py                  # (Optional) helper functions
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_preprocess.py        # Unit tests for preprocessing logic
â”‚
â”œâ”€â”€ README.md                     # Project overview and instructions
â”œâ”€â”€ problem.md                    # Provided project prompt
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
```

---

##  How It Works

###  Description Normalization

* Lowercased all text
* Removed or replaced inline date patterns like `* 14 Oct` â†’ `[date]`

###  Token Replacement Strategy

| Token          | Replaces Examples             | Purpose                      |
| -------------- | ----------------------------- | ---------------------------- |
| `[grocers]`    | Woolworths, Checkers, PnP     | Reduces vendor name noise    |
| `[restaurant]` | KFC, Uber Eats, Marble Pantry | Generalizes fast food/dining |
| `[garage]`     | Engen, Sasol, Shell           | Unifies fuel vendors         |
| `[location]`   | Bryanston, Arcadia, Craighall | Removes branch/region noise  |
| `[date]`       | `* 12 Oct`, `* 15 Sep`        | Removes date-specific tokens |

###  Location & Type Inference

* Inserted `[location]` between `[restaurant]`/`[garage]` and `[date]`
* Inferred `[restaurant]` when missing for known `Eating Out` transactions

---

## ğŸ“Š Model Performance (Raw vs Tokenized)

| Metric            | Raw Description | Tokenized | Improvement |
| ----------------- | --------------- | --------- | ----------- |
| Accuracy          | 0.75            | 0.84      |  +0.09     |
| Macro F1-Score    | 0.52            | 0.60      |  +0.08     |
| Weighted F1-Score | 0.69            | 0.79      |  +0.10     |

Tokenization notably improved performance for:

* `Eating Out`: F1 â†‘ from 0.69 â†’ 1.00
* `Groceries`: F1 â†‘ from 0.71 â†’ 0.91
* `Fuel`: F1 â†‘ from 0.00 â†’ 1.00
* `General Purchases`: F1 â†‘ from 0.40 â†’ 0.67

---

##  Testing

Run unit tests for the preprocessing logic:

```bash
pytest tests/test_preprocess.py
```

Covers:

* Grocer name substitution
* Inferred `[restaurant]` token
* Location detection for Fuel/Eating Out

---

##  Running the Tokenizer

```python
from src.preprocess import replace_tokens

df['description'] = df.apply(lambda row: replace_tokens(row['description'], row['label']), axis=1)
df.to_csv("data/transactions_tokenized.csv", index=False)
```

---

##  Writeup

For a complete explanation of:

* Design decisions
* Assumptions
* Implementation breakdown
* Model insights

â¡ï¸ See `report/writeup.md`

---

## ğŸ‘¤ Author

Developed by **Yassir Ali**
As part of a Data Science test assessment.

> â€œTokenizing noise into meaning.â€

